% This chapter should describe what was actually produced: the programs which were written, the hardware which was built or the theory which was developed. Any design strategies that looked ahead to the testing stage should be described in order to demonstrate a professional approach was taken.

% Descriptions of programs may include fragments of high-level code but large chunks of code are usually best left to appendices or omitted altogether. Analogous advice applies to circuit diagrams or detailed steps in a machine-checked proof.

% The implementation chapter should include a section labelled "Repository Overview". The repository overview should be around one page in length and should describe the high-level structure of the source code found in your source code repository. It should describe whether the code was written from scratch or if it built on an existing project or tutorial. Making effective use of powerful tools and pre-existing code is often laudable, and will count to your credit if properly reported. Nevertheless, as in the rest of the dissertation, it is essential to draw attention to the parts of the work which are not your own. 

% It should not be necessary to give a day-by-day account of the progress of the work but major milestones may sometimes be highlighted with advantage.

% practical contributions.
% present deadlocks \& performance tests.
% reusable modules.

% Live testing revealed many subtle bugs, deadlocks, and performance issues (memory usage, messages dropped due to bugs preventing leaders from always advancing) that were time consuming to debug. However they helped me to flesh out a pacemaker algorithm based on these ad-hoc fixes that we have proven both correctness and liveness for. I also improved the ease of implementability based on practicalities I discovered during implementation (eg. using node offset and hashes to compare nodes)

% --- directory forest stuff
\definecolor{foldercolor}{RGB}{124,166,198}

\tikzset{pics/folder/.style={code={%
	\node[inner sep=0pt, minimum size=#1](-foldericon){};
	\node[folder style, inner sep=0pt, minimum width=0.3*#1, minimum height=0.6*#1, above right, xshift=0.05*#1] at (-foldericon.west){};
	\node[folder style, inner sep=0pt, minimum size=#1] at (-foldericon.center){};}
	},
	pics/folder/.default={20pt},
	folder style/.style={draw=foldercolor!80!black,top color=foldercolor!40,bottom color=foldercolor}
}

\forestset{is file/.style={edge path'/.expanded={%
		([xshift=\forestregister{folder indent}]!u.parent anchor) |- (.child anchor)},
		inner sep=1pt},
	this folder size/.style={edge path'/.expanded={%
		([xshift=\forestregister{folder indent}]!u.parent anchor) |- (.child anchor) pic[solid]{folder=#1}}, inner ysep=0.6*#1},
	folder tree indent/.style={before computing xy={l=#1}},
	folder icons/.style={folder, this folder size=#1, folder tree indent=3*#1},
	folder icons/.default={12pt},
}
% ---

\section{Overview} \label{overview}
% add diagram here
%Present main program structure by modules: the consensus state machine and its interface, the server code \& main loop (including stream), communication schema
% some of this is inspired by the OCons project, it was still in development
The core implementation of the HotStuff algorithm (which we will give in \ref{spec}) is implemented in the \textit{consensus} module. The main function provided by this module is \textit{advance}, which delivers some event (such as an incoming message, client request or timeout) to the consensus state machine and returns an updated state and a list of actions (such as sending a message to another node or responding to a client request) to be carried out. This architecture is inspired by the \href{https://github.com/Cjen1/OCons}{OCons }project\footnote{At the time I began implementation the OCons project was still under development, so I was unable to use the code in my project.}, which was developed by my project supervisor. The \textit{consensus} module contains both a chained and unchained implementation that share a common signature, so can be interchanged. The module uses the Tezos cryptography library (see \ref{tezos}) for signing messages, aggregating signatures, and checking quorum certificates.

Each node operates as a server waiting for messages from other nodes or requests from a client (or in the case of our experiments a load generator, which is described in \ref{benchcode}). The format of RPCs is specified in a Cap'n Proto schema, in their \href{https://capnproto.org/language.html}{custom markdown language}. A received RPC must be decoded, and the Cap'n Proto types converted into the internal types of the consensus state machine. Messages and requests are added to separate streams\footnote{A stream is thread-safe implementation of a queue in Lwt.} when they are received. When a request is received the callback function to respond to the request is stored in a hash table, so that it can be accessed when the command has been committed and the client request can be responded to.

The main loop takes events from the message and request streams, prioritising internal messages over client requests [expand on why, maybe cite something ***]. It takes these events and delivers them to the \textit{advance} function of the consensus state machine. This architecture was chosen so that the \textit{advance} function is never run in parallel on different messages / requests, as this could lead to race conditions. The \textit{advance} function then returns a new state which is stored, and a list of actions to carry out.

The actions that can be carried out are sending a message to other nodes, responding to a client request, and resetting a timer. In order to send a message we must convert the consensus state machine internal types into Cap'n Proto types, and construct an RPC that matches the schema. Messages are dispatched asynchronously in a new thread. The node maintains TCP connections with all other nodes that are reused every time a message is sent, and in the event of the connection breaking the node repeatedly attempts to reconnect with binary exponential back-off. When responding to a client request the committed command's unique identifier is used to lookup the callback to respond to the client, which is then called, sending a response to the client. The timer is implemented with Lwt promises, a new thread is created which waits for a timeout to elapse and then adds a TIMEOUT message to the message stream.

\section{Pacemaker Specification} \label{spec}
We present the pseudocode for the unchained algorithm given in the HotStuff paper (see algorithm 4 \& 5) with our additions coloured in green and modifications in pink. We have only shown the main changes and not the other features and performance improvements we have made (such as batching), we will present these in \ref{deadlock}. In order to better map to the original pseudocode we break the algorithm into HotStuff and the pacemaker, although in our modified algorithm these two sections are not cleanly separable.

\begin{algorithm}[h!]
	\caption{Modified HotStuff}\label{hotstuff}
	\begin{algorithmic}[1]
	\color{Magenta}
	\Function{CREATE{\large L}EAF}{\textit{parent, cmd, qc}}
		\State $\textit{b.parent} \gets \text{branch extending with dummy nodes from}\ \textit{parent}\ \text{to height}\ \textit{curView}$
		\State $\textit{b.height} \gets \textit{curView} + 1$
		\State $\textit{b.cmd} \gets \textit{cmd}$
		\State $\textit{b.justify} \gets \textit{qc}$
		\State \textbf{return} b
	\EndFunction
	\color{black}
	\Procedure{UPDATE}{\textit{b\textsuperscript{*}}}
		\State $\textit{b''} \gets \textit{b\textsuperscript{*}.justify.node}$
		\State $\textit{b'} \gets \textit{b''.justify.node}$
		\State $\textit{b} \gets \textit{b\textsuperscript{*}.justify.node}$
		\State $\text{UPDATE{\large Q}C{\large H}IGH}(\textit{b\textsuperscript{*}.justify})$
		\If{$\textit{b'.height} > \textit{b\textsubscript{lock}.height}$}
			\State $\textit{b\textsubscript{lock}} \gets \textit{b'}$
		\EndIf
		\If{$(\textit{b''.parent} = \textit{b'}) \land (\textit{b'.parent} = \textit{b})$}
			\State $\text{ON{\large C}OMMIT}(\textit{b})$
			\State $\textit{b\textsubscript{exec}} \gets \textit{b}$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large C}OMMIT}{\textit{b}}
		\If{$\textit{b\textsubscript{exec}.height} < \textit{b.height}$}
			\State $\text{ON{\large C}OMMIT}(\textit{b.parent})$
			\State $\text{EXECUTE}(\textit{b.cmd})$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large P}ROPOSAL}{$\text{MSG}\textsubscript{\textit{v}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \bot)$}
		\color{Green}
		\If {$ \textit{m.view} = curView$}
			\color{black}
			\If{$\textit{b\textsubscript{new}.height} > \textit{vheight} \land (\textit{b\textsubscript{new}}\ \text{extends}\ \textit{b\textsubscript{lock}} \lor \textit{n.height} > \textit{b\textsubscript{lock}.height})$}
				\State $\textit{vheight} \gets \textit{b\textsubscript{new}.height}$
				\State $ \text{SEND}(\text{GET{\large L}EADER}(), \text{VOTE{\large M}SG}\textsubscript{\textit{u}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \bot))$
			\EndIf
			\State $\text{UPDATE}(\textit{b\textsubscript{new}})$
			\color{Green}
			\State $\text{SEND}(\text{GET{\large N}EXT{\large L}EADER}(), \text{MSG}\textsubscript{\textit{u}}(\text{NEW{\large V}IEW}, \bot, \textit{qc\textsubscript{high}}))$
			\If{$ \text{not}\ \text{IS{\large N}EXT{\large L}EADER}() $}
				\State $\text{ON{\large N}EXT{\large S}YNC{\large V}IEW}(\textit{curview} + 1)$
			\EndIf
		\EndIf
		\color{black}
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large V}OTE}{$\text{VOTE{\large M}SG}\textsubscript{\textit{v}}(\text{GENERIC{\large A}CK}, \textit{b}, \bot)$}
		\color{Green}
		\If {$ \text{IS{\large L}EADER}(\textit{m.view} + 1) \land \textit{m.view} \ge curView$}
			\color{black}
			\If {$\exists(v, \sigma') \in V\textcolor{Magenta}{\textsubscript{m.view}}[b]$}
				\State \textbf{return}
			\EndIf
			\State $V[b] \gets V\textcolor{Magenta}{\textsubscript{m.view}}[b] \cup \{(v, m.partialSig)\}$
			\If{$ |V\textcolor{Magenta}{\textsubscript{m.view}}[b]| \ge n - f$}
				\State $qc \gets QC(\{ \sigma | (v', \sigma) \in V\textcolor{Magenta}{\textsubscript{m.view}}[b]\})$
				\State $ \text{UPDATE{\large Q}C{\large H}IGH}(\textit{qc}) $
				\color{Green}
				\State $\text{ON{\large N}EXT{\large S}YNC{\large V}IEW}(\textit{m.view} + 1)$
				\color{black}
			\EndIf
		\EndIf
	\EndProcedure
	\Function{ON{\large P}ROPOSE}{$ \textit{b\textsubscript{leaf}}, \textit{cmd}, \textit{qc\textsubscript{high}}$}
		\State $b\textsubscript{new} \gets \text{CREATE{\large L}EAF}(\textit{b\textsubscript{leaf}, \textit{cmd}, \textit{qc\textsubscript{high}}, \textit{b\textsubscript{leaf}.height} + 1})$
		\State $ \text{BROADCAST}(\text{MSG}\textsubscript{\textit{v}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \bot)) $
		\State \textbf{return} b\textsubscript{new}
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{Modified Pacemaker}\label{pacemaker}
	\begin{algorithmic}[1]
	\Function{GET{\large L}EADER}{}
		\color{Green}
		\State $\textbf{return}\ \textit{curView}\ \text{mod}\ \textit{nodeCount}$
		\color{black}
	\EndFunction
	\Procedure{UPDATE{\large Q}C{\large H}IGH}{$\textit{qc'\textsubscript{high}}$}
		\If {$\textit{qc'\textsubscript{high}.node.height} > \textit{qc\textsubscript{high}} $}
			\State $ \textit{qc'\textsubscript{high}} \gets \textit{qc\textsubscript{high}} $
			\State $ \textit{b\textsubscript{leaf}} \gets \textit{qc'\textsubscript{high}.node}$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large B}EAT}{$\textit{cmd}$}
		\If {$ \textit{u} = \text{GET{\large L}EADER}()$}
			\State $ \textit{b\textsubscript{leaf}} \gets \text{ON{\large P}ROPOSE}(\textit{b\textsubscript{leaf}}, \textit{cmd}, \textit{qc\textsubscript{high}})$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large N}EXT{\large S}YNC{\large V}IEW}{$\textit{view}$}
		\color{Magenta}
		\State $ \textit{curView} \gets \textit{view}$
		\State $ \text{ON{\large B}EAT}(\textit{cmds.take()})$
		\State $ \text{RESET{\large T}IMER}(\textit{curView})$
		\color{black}
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large N}EW{\large V}IEW}{$ \text{MSG}\textsubscript{\textit{u}}(\text{NEW{\large V}IEW}, \bot, \textit{qc'\textsubscript{high}}) $}
		\State $ \text{UPDATE{\large Q}C{\large H}IGH}(\textit{qc'\textsubscript{high}}) $
	\EndProcedure
	\color{Green}
	\Procedure{ON{\large R}ECIEVE{\large C}LIENT{\large R}EQUEST}{\text{REQ}(\textit{cmd})}
		\State $ \textit{cmds.add(cmd)} $
	\EndProcedure
	\Procedure{ON{\large T}IMEOUT}{$ \textit{view} $}
		\State $ \text{SEND}(\text{GET{\large N}EXT{\large L}EADER}(), \text{MSG}(\text{COMPLAIN}, \bot, \bot)) $
		\State $ \text{RESET{\large T}IMER}(\textit{view} + 1) $
	\EndProcedure
	\Procedure{ON{\large R}ECIEVE{\large C}OMPLAIN}{$ m = \text{MSG}(\text{COMPLAIN}, \bot, \bot) $}
		\If {$ \text{IS{\large L}EADER}(\textit{m.view} + 1) \land \textit{m.view} \ge \textit{curView}$}
			\If {$\exists(v, \sigma') \in C\textsubscript{m.view}[b]$}
				\State \textbf{return}
			\EndIf
			\State $C\textsubscript{m.view}[b] \gets C[b] \cup \{(v, m.partialSig)\}$
			\If{$ |C\textsubscript{m.view}[b]| = n - f$}
				\State $qc \gets QC(\{ \sigma | (v', \sigma) \in C\textsubscript{m.view}[b]\})$
				\State $\text{BROADCAST}(\text{MSG}(\text{NEXT{\large V}IEW}, \bot, \textit{qc}))$
			\EndIf
		\EndIf
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large N}EXT{\large V}IEW}{$ m = \text{MSG}(*, \bot, \textit{qc}) $}
		\If {$ \textit{qc.view} \ge \textit{curView}$}
				\State $\text{ON{\large N}EXT{\large S}YNC{\large V}IEW}(\textit{qc.view} + 1)$
		\EndIf
	\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{itemize}
	\item CREATE{\large L}EAF (Algorithm \ref{hotstuff}): This function has been modified so that `dummy' nodes are inserted to maintain the invariant that the height of the chain is always one greater than the current view number. This is very similar to the CREATE{\large L}EAF function given for the chained version of the protocol in the HotStuff paper; it is unclear as to why this was changed for the unchained version [work out why it changed?].
	\item ON{\large R}ECEIVE{\large P}ROPOSAL (Algorithm \ref{hotstuff}): The change on line 22 ensures that we only respond to a proposal from our current view, this is important for safety but was not explicitly included in the original pseudocode. The change on line 27 is that we send a NEW{\large V}IEW message to the next leader once we receive a proposal. This is in contrast to the original pseudocode where we send a NEW{\large V}IEW inside ON{\large N}EXT{\large S}YNC{\large V}IEW on receiving some unspecified interrupt. Finally, once we have received a proposal we can transition into the next view \textit{unless} we are the next leader, in which case we must wait to collect the VOTE{\large M}SGs before transitioning.
	\item ON{\large R}ECEIVE{\large V}OTE (Algorithm \ref{hotstuff}): The change on line 31 ensures we ignore messages from earlier views, which is important for liveness, and that we are the correct destination for the vote message. Another change we make is dividing \textit{V} into different sets for messages from different views; this prevents votes from different views being used to form a quorum. The change on line 38 means that once a leader has received a quorum of messages, it can transition to the next view (which it starts by sending a proposal in ON{\large N}EXT{\large S}YNC{\large V}IEW)
	\item GET{\large L}EADER (Algorithm \ref{pacemaker}): The original pseudocode states that this function is application specific. We have chosen to use a round-robin system to assign leaders to views.
	\item ON{\large N}EXT{\large S}YNC{\large V}IEW (Algorithm \ref{pacemaker}): This function previously just sent a NEW{\large V}IEW message to the next leader. Our modified function updates the \textit{curView}, and resets the timer for the new view. Additionally it calls ON{\large B}EAT which causes a leader to propose a new value.
	\item ON{\large R}ECIEVE{\large C}LIENT{\large R}EQUEST (Algorithm \ref{pacemaker}): On receiving a client request we simply add it to a queue of commands waiting to be proposed.
	\item ON{\large T}IMEOUT (Algorithm \ref{pacemaker}): When our current view times out we send a COMPLAIN to the next leader, and reset our timer for the next view. This behaviour is explained in \ref{viewchange}.
	\item ON{\large R}ECIEVE{\large C}OMPLAIN (Algorithm \ref{pacemaker}): This function is very similar to ON{\large R}ECEIVE{\large V}OTE, except that we are collecting a quorum of COMPLAIN messages rather than votes. On achieving a quorum we can broadcast a NEXT{\large V}IEW message to get the replicas to transition to the next state, including the quorum of COMPLAINs as proof. This behaviour is explained in \ref{viewchange}.
	\item ON{\large R}ECEIVE{\large N}EXT{\large V}IEW (Algorithm \ref{pacemaker}): N.B. the message type given is the wildcard operator, so this procedure is run on every message we receive. The function checks if the quorum received is from a greater view than \textit{curView}, if so then it is safe to transition to that view in order to catch up.
\end{itemize}

\subsection{Correctness Proof}

\subsection{Liveness Proof}
%prove that it doesn't break safety
%maybe try to prove liveness
% https://decentralizedthoughts.github.io/2021-09-30-distributed-consensus-made-simple-for-real-this-time/
\section{Performance improvements} \label{deadlock}
% Batching \& limiting of batch sizes. deduplication in batching
% Send to all

% We give cases of deadlock conditions
% and performance issues / improvements found from debugging traces:
% 1. print statements (even when not in-between timing commands) can affect the time measured in another operation. Removed prints and printed all results at the end (use static collector function for logging that is passed around the program).
% 2. >= used lexicographic ordering, resulted in chains being verified due to first field being increasing until a specific command "20" where the value became lexicographically decreasing
% 3. Infinitely recursive start node not compatible nice with messaging system
% 4. Memory usage massively increased due to recursive node functions, replaced with an offset
% 5. Store a hash in each node and use it to compare nodes rather than checking their entire history
% 6. Discarding messages from future views results in some leaders not progressing
\subsection{Batching}
% mention send to all & send to one, 4x throughput
% reference latency graphs from final
One way to improve goodput (number of requests committed) is to `batch' requests, meaning a node may contain many commands instead of just one. This can dramatically increase goodput as now a single view can result in many commands being committed and executed instead of just one.

In order to implement this change in algorithm \ref{pacemaker} one simply has to modify the ON{\large N}EXT{\large S}YNC{\large V}IEW procedure. Instead of taking a single element from the queue of commands waiting to be proposed, the whole queue will be `batched' into a single proposal.

In theory this change should result in significantly higher goodput without any significant increase in latency. Analysis of timing data after implementing this feature revealed that latency had increased substantially. We now present some of the analysis and experimentation that was carried out to diagnose this issue. As mentioned in \ref{testing}, the nature of the project meant that debugging had to be carried out by manual inspection of the program trace and timing sections of the program. To overcome this I carried out tests in a scientific manner, constructing a hypothesis for why the program was slow based on crawling through logs, then attempting to test my hypothesis while controlling other variables, and finally implementing a solution.

\subsubsection{Probing effects}
% development of logging framework, some details of what is logged
I started by increasing the number of timing and logging statements in key parts of the program, allowing me to better diagnose the source of the poor performance. Running a live test with print statements has the advantage that one can quickly see when progress is not being made, or when there are pauses, as the print statements stop. However, this benefit is outweighed by the sheer volume of logs and times being printed, it becomes difficult to manage when logs are in the millions of lines long.

Moreover, my debugging by print statements had a larger problem of large inconsistencies between runs making it difficult to diagnose any problem. After some experimentation I realised that my print statements had a significant effect on the performance of the program. I had assumed that because the print statements were not in-between the timing statements they would not affect my measurements, but they are an expensive operation that can cause delays to happen in execution where one would not expect.

In order to overcome this problem I developed a simple logging framework. Key parts of the program such as the state machine advancing, actions being carried out, messages being sent, are all timed and added to lists. Critically, this list is stored and not printed out until the node is killed, so the printing cannot interfere with execution. Relevant statistics such as mean, standard deviation, and total time for each interval measured are outputted, providing crucial insight into the performance of the program.

\subsubsection{Architectural experimentation}
% https://outlook.office.com/mail/id/AAQkADc5ODY2YTdlLTQ3NGMtNDVmOS05ZDAxLWNhYzkwMjU2MThhZAAQADc%2B0AASzwxBgZSNOjdNZ%2Bw%3D
% development of architecture in overview
% tried moving to non event-driven model
% thought resource contention was issue, in fact it was async get function
My initial analysis of timing data showed a large amount of time was spent by requests queuing on the node before they are delivered to the state machine. At this time I had not developed the whole of the architecture presented in \ref{overview}, specifically, I only had one stream that contained both internal messages and client requests. This led me to develop the following:

\textbf{Hypothesis: } Internal messages are being starved by client requests. At large throughputs the stream quickly becomes filled with client requests, which may prevent internal messages being handled. Internal messages represent a backlog of work that we have not yet finished, so this should be handled before accepting more work (client requests).

\textbf{Potential solution: } Split the stream into two separate streams for messages and requests, and always pick from the message stream over the request stream until the message stream is empty.

Implementing the potential solution did not lead to a significant improvement in performance, so this was not the issue. However, I retained this architectural model as it is theoretically better and may lead to observable improvements later. Next, I began to think that the actual cause of the issue may be that internal messages are actually starving client requests. My implementation of the pacemaker (\ref{spec}) immediately begins a new view as soon as the previous one is finished, which led me to make the following hypothesis.

\textbf{Hypothesis: } Because the state machine is constantly advancing at the fastest possible rate, the volume of internal messages may prevent new requests from being handled.

\textbf{Experimentation: } I attempted to `balance' the starvation by mostly prioritising internal messages, but every \textit{x} iterations picking a request instead of an internal message. Varying \textit{x} led to significant changes in performance, but I could not find any value that led to a good level of performance. I chose to pursue another route, as the nature of starvation seemed to be a complicated, with messages starving requests, and the other way around.

\textbf{Potential solution: } Instead of starting the next view whenever possible, deliver the \textit{BEAT} and \textit{ON{\large N}EXT{\large S}YNC{\large V}IEW} interrupts at a steady rate.

I implemented this potential solution, which involved checking a timer on every iteration of the main loop to see if some $\Delta$ had elapsed, and it was time to deliver the next interrupt; it also required significant modifications to be made to the consensus state machine. Implementing this feature actually led to a significant performance hit, so I discarded the changes I made. However, while experimenting with this feature I noticed that the main loop which is infinitely recursive, was running at a much slower rate than one would expect. Further timing statements revealed that the command which removed an element from the stream would sometimes take a very long time (on the order of seconds). Reading more into the Lwt documentation led me to the following idea:

\textbf{Potential solution: } Use a different method to take elements from the queue that is synchronous. This should stop the main loop blocking waiting for the queue to fill up.

This change improved performance by reducing the amount of time spent waiting in the main loop, the difference is shown in an ablation study [reference evaluation ***], and I hoped that the problem had been fixed.

\subsubsection{Send to all and deduplication}
% send to all weird graph: https://outlook.office.com/mail/id/AAQkADc5ODY2YTdlLTQ3NGMtNDVmOS05ZDAxLWNhYzkwMjU2MThhZAAQAM2hZuzkpUNIuGt2QHNZQvc%3D
% exponential growth with end to all https://imgur.com/a/wDlhbn9 28/03 pseudocode
% theory: nodes reproposing same values
% first solution: deduplicate before commiting, use set instead of queue
% second solution: deduplicate based on *seen* commands
% could mention bug of reawaking the same promise in the hash table - this brought to my attention the deduplication issue!
\subsubsection{Batch sizes}
% insight: track batch sizes
% experimented with limiting batch size
% tie into benchmarking of cap'n proto
\subsection{Encoding of nodes}
% https://outlook.office.com/mail/id/AAQkADc5ODY2YTdlLTQ3NGMtNDVmOS05ZDAxLWNhYzkwMjU2MThhZAAQALSHx8XQgvxMrG5AvVEd9as%3D
% 19/01 onwards
% first issue: infintely recursive node structure
% solution: hardcoding final node at either end so it is not sent
% second issue: growing memory usage - revealed by profiling
% solution: use offset to node, and reconstruct node at either end. introduce `justify' type
% third issue: memory usage comparing nodes
% solution: include node hashes
% fourth issue: size of sending all nodes
% solution: TODO implement TCP
\subsection{Connections}
% talk about capabilities?

\section{Verifiable anonymous identities}
% https://www.usenix.org/system/files/nsdi22-paper-shamis.pdf
% section 5
% when to set up connections? use promises
% https://outlook.office.com/mail/id/AAQkADc5ODY2YTdlLTQ3NGMtNDVmOS05ZDAxLWNhYzkwMjU2MThhZAAQAEXEcHtcUNFFo%2FRcUtQcSmc%3D

\section{Implementing for evaluation} \label{benchcode}
Benchmarking code
discuss open loop vs closed loop clients
avoid coordinated ommision by using open loop clients
% section 3: https://dl.acm.org/doi/pdf/10.1145/3447851.3458739
\section{Repository Overview}

\begin{forest}
	for tree={font=\sffamily, grow'=0,
	folder indent=0.9em, folder icons,
	edge=densely dotted}
	[src
		[\ consensus,
			[consensus\_chained\_impl.ml, is file]
			[consensus\_chained\_test.ml, is file]
			[consensus\_impl.ml, is file]
			[consensus\_test.ml, is file]
			[consensus.ml, is file]
			[consensus.mli, is file]
			[crypto.ml, is file]
			[types.ml, is file]
			[types.mli, is file]
			[util.ml, is file]
		]
		[\ experiments
			[\ data]
			[\ graphs]
			[plot.py, is file]
			[plotDummy.py, is file]
		]
		[hs\_api.capnp, is file]
		[api\_wrapper.ml, is file]
		[api\_wrapper.mli, is file]
		[dummy.ml, is file]
		[hs.ml, is file]
		[live\_test.ml, is file]
		[main.ml, is file]
		[net.ml, is file]
		[types.ml, is file]
		[util.ml, is file]
		[runDummyExperiments.py, is file]
		[runExperiments.py, is file]
	]
\end{forest}

The \textit{src} directory contains the main server loop and the code for interacting with Cap'n Proto to send messages. It also contains code for the load generator, and Python scripts to run experiments and benchmarks. Inside the \textit{consensus} folder is the implementation of the consensus library based on the pseudocode presented in \ref{spec}. The \textit{experiments} folder is where the data from running experiments is outputted to, and it contains scripts for plotting graphs.