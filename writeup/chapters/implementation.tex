% This chapter should describe what was actually produced: the programs which were written, the hardware which was built or the theory which was developed. Any design strategies that looked ahead to the testing stage should be described in order to demonstrate a professional approach was taken.

% Descriptions of programs may include fragments of high-level code but large chunks of code are usually best left to appendices or omitted altogether. Analogous advice applies to circuit diagrams or detailed steps in a machine-checked proof.

% The implementation chapter should include a section labelled "Repository Overview". The repository overview should be around one page in length and should describe the high-level structure of the source code found in your source code repository. It should describe whether the code was written from scratch or if it built on an existing project or tutorial. Making effective use of powerful tools and pre-existing code is often laudable, and will count to your credit if properly reported. Nevertheless, as in the rest of the dissertation, it is essential to draw attention to the parts of the work which are not your own. 

% It should not be necessary to give a day-by-day account of the progress of the work but major milestones may sometimes be highlighted with advantage.

% practical contributions.
% present deadlocks \& performance tests.
% reusable modules.

% Live testing revealed many subtle bugs, deadlocks, and performance issues (memory usage, messages dropped due to bugs preventing leaders from always advancing) that were time consuming to debug. However they helped me to flesh out a pacemaker algorithm based on these ad-hoc fixes that we have proven both correctness and liveness for. I also improved the ease of implementability based on practicalities I discovered during implementation (eg. using node offset and hashes to compare nodes)

% --- directory forest stuff
\definecolor{foldercolor}{RGB}{124,166,198}

\tikzset{pics/folder/.style={code={%
	\node[inner sep=0pt, minimum size=#1](-foldericon){};
	\node[folder style, inner sep=0pt, minimum width=0.3*#1, minimum height=0.6*#1, above right, xshift=0.05*#1] at (-foldericon.west){};
	\node[folder style, inner sep=0pt, minimum size=#1] at (-foldericon.center){};}
	},
	pics/folder/.default={20pt},
	folder style/.style={draw=foldercolor!80!black,top color=foldercolor!40,bottom color=foldercolor}
}

\forestset{is file/.style={edge path'/.expanded={%
		([xshift=\forestregister{folder indent}]!u.parent anchor) |- (.child anchor)},
		inner sep=1pt},
	this folder size/.style={edge path'/.expanded={%
		([xshift=\forestregister{folder indent}]!u.parent anchor) |- (.child anchor) pic[solid]{folder=#1}}, inner ysep=0.6*#1},
	folder tree indent/.style={before computing xy={l=#1}},
	folder icons/.style={folder, this folder size=#1, folder tree indent=3*#1},
	folder icons/.default={12pt},
}
% ---
This chapter describes the architecture of my implementation (Section~\ref{architecture}), concludes my theoretical explanation of HotStuff by discussing the chained algorithm and the pacemaker (Section~\ref{moretheory}), presents a full specification for HotStuff with a proof of correctness and liveness (Section~\ref{spec}), describes key optimisations implemented (Section~\ref{performance}), presents the load generator and experiment scripts which will be used in evaluation (Section~\ref{benchcode}), and gives an overview of the repository structure (Section~\ref{repo}).

\section{System Architecture} \label{architecture}

\begin{figure}[h!]
\centering
\includegraphics[scale=0.925]{nodediagram.pdf}
\caption{Architecture of a node.}
\label{nodediagram}
\end{figure}

This section presents the system architecture that surrounds the core \textit{consensus} module, passing it new messages and client requests, and allowing it to send messages and respond to the client. This architecture is inspired by the OCons project\footnote{At the time I began implementation the OCons project was still under development, so I was unable to use the code in my project.}~\cite{ocons}, which was developed by my project supervisor.

We will follow the path of an incoming request or message travelling through the system, as shown in Figure~\ref{nodediagram}.

\begin{description}
	\item \textit{Incoming message and client RPCs} --- the node responds to internal messages from other nodes, and requests from a client (or a load generator, as described in Section~\ref{loadgenerator}) containing new commands to be committed. The format of these RPCs is specified in a Cap'n Proto schema, in their custom markdown language.
	\item \textit{Server} --- each node operates as a server waiting for incoming RPCs. When the server receives an RPC it must be converted from Cap'n Proto types to internal types, and added to the \textit{message stream} or \textit{request stream}. If the RPC was a client request, a promise for a response is added to the \textit{client response hash table}; this will allow the system to respond to the client once the command is decided.
	\item \textit{Message and request streams\footnote{A stream is a thread-safe implementation of a queue in Lwt.}} --- messages and requests are added to separate streams so that messages can be prioritised. Internal messages represent a backlog of work that the system has not yet completed, so we follow the general design principle of clearing this backlog before accepting new work (client requests).
	\item \textit{Main loop} --- takes messages and requests from their respective streams, and delivers them to the \textit{consensus} module. This main loop ensures that the \textit{consensus} module is never run in parallel, which could lead to race conditions.
	\item \textit{Consensus module} --- takes incoming messages and requests, outputs \textit{actions} such as sending messages, and updates its own state. The module contains an implementation of the basic HotStuff algorithm (Section~\ref{hotstufftheory}), and the chained algorithm (chaining is discussed in Section~\ref{chaining}, and a full specification is given in Section~\ref{spec}); both share the same signature, so can be interchanged. The module uses the Tezos cryptography library (Section~\ref{tezos}) for signing messages, aggregating signatures, and checking quorum certificates.
	\item \textit{Action handler} --- takes the actions output by the \textit{consensus} module, and passes them to the appropriate handler. The three types of actions are: responding to the client, sending a message, and resetting the view timer.
	\item \textit{Client request hashtable} --- allows client requests to be responded to. The hashtable maps each command's unique identifier to a promise that will be awoken to respond to the original client request RPC once the command is committed.
	\item \textit{Message sender} --- asynchronously dispatches an RPC in a new thread. To do this it must convert internal types into Cap'n Proto types and construct an RPC that matches the schema. The message sender maintains TCP connections with all other nodes, and in the event of the connection breaking repeatedly attempts to reconnect with binary exponential back-off times.
	\item \textit{View timer} --- waits for a timeout to elapse then adds a view timeout message to the \textit{message stream} so that it will be delivered to the \textit{consensus} module. The \textit{reset} action allows the timer to be reset for a new view.
\end{description}

\section{More HotStuff theory} \label{moretheory}
This section concludes my explanation of HotStuff by discussing chaining and the pacemaker. As the pacemaker was not specified in the original paper, I draw on other sources to give an explanation.

\subsection{Chaining} \label{chaining}

This section describes the chained HotStuff algorithm, which is an optimised version of the basic algorithm described in Section~\ref{hotstufftheory} where different phases are pipelined. This is a standard optimisation for consensus algorithms that is described in the original paper~\cite{yinHotStuffBFTConsensus2019}.

Pipelined phases both simplify and optimise the basic algorithm. The phases in the basic algorithm are similar; they involve collecting votes from replicas to form a QC. Each of the four phases\footnote{Excluding the \textit{new-view} phase which is unchanged.} can be carried out concurrently. In each view, a leader collects votes to form a QC which can serve in all of the concurrent phases, then sends this QC to the next leader in a \textit{new-view} message and broadcasts a proposal to the nodes. 

%Instead of having different phases as before, we can have a single \textit{generic} phase that collects votes, creates a \textit{generic QC}, and sends it to the next leader; now each view is the length of a single phase and a QC can serve in multiple phases concurrently. The only exception to this is the \textit{new-view} phase which is the same as in the basic algorithm.

\begin{figure}[h!]
\centering
\includegraphics[]{nchain.pdf}
\caption{Sequence of nodes forming a 2-chain.}
\label{nchain}
\end{figure}

In each proposal, a chain of nodes (equivalent to the log) is extended, and different phases are carried out on suffixes of the chain depending on their length. For example, Figure~\ref{nchain} shows a 2-chain, which means a proposal has already been through 2 phases; this is the equivalent of being in the \textit{commit} phase in the basic algorithm.

\subsection{Pacemaker} \label{pacemaker}

This section discusses the pacemaker, which synchronises the views of honest replicas: a necessary condition for liveness (Property \ref{livenessproperty}). The original paper did not specify the pacemaker mechanism, so I have synthesised information from different sources (including experimentation) to give a full specification of HotStuff with a pacemaker in Section~\ref{spec}. Part of the pacemaker that will be explained in this section is the view-change protocol, which allows the view of a faulty leader to be skipped; ours is based on the pacemaker for LibraBFT~\cite{baudetStateMachineReplication2019, ittai}. The discussion will also cover the integration of the pacemaker with the HotStuff algorithm, which was achieved through modifications aimed at resolving deadlocks encountered during implementation.

There are two properties that a pacemaker must possess to provide the conditions needed for liveness~\cite{naorCogsworthByzantineView2021}; it will later be proven that my pacemaker has these properties (Section~\ref{proof}). The first property, \textit{view synchronisation} (Theorem \ref{viewsync}), ensures that there are an infinite number of views with an honest leader that the non-faulty nodes stay in for long enough to make progress. The second property, \textit{synchronisation validity} (Theorem \ref{syncvalid}), ensures that the pacemaker will only advance to the next view if some honest replica wishes it to happen.

Pacemakers are related to failure detectors: mechanisms that facilitate the detection of failed nodes~\cite{chandraWeakestFailureDetector1996,chandraUnreliableFailureDetectors1996}. A pacemaker extends this idea, allowing it to be used with rotating leaders.

\subsubsection{View-change protocol} \label{viewchange}

Once the timeout for a view elapses, nodes send a \textit{complain} message to the next leader and start a new timeout for the next view. Once the next leader achieves a quorum of \textit{complain} messages it collects them into a QC known as a \textit{view-change proof}. This leader can then send a \textit{view-change} message containing the \textit{view-change proof} to all replicas, who respond by transitioning to the next view and sending a \textit{new-view} message to the new leader. The inclusion of the \textit{view-change proof} prevents liveness attacks by byzantine nodes that could otherwise attack the system by constantly causing view-changes to take place and preventing non-faulty leaders from making progress.

Crucially this protocol maintains the linear view change property that HotStuff has $O(n)$ authenticator complexity. Authenticator complexity measures the total number of threshold signatures and partial signatures in a view. This protocol requires $n - f$ partial signatures for the \textit{complain} messages, and a single threshold signature for the \textit{view-change} message, resulting in $O(n)$ authenticators overall.

\subsubsection{Integrating the pacemaker with HotStuff}

My approach to integrating the pacemaker is to advance a node to the next view as soon as possible once it has finished proposing or voting, or when it receives evidence that there is a quorum of nodes in a higher view. This means that the system does not require synchronised clocks; the nodes advance asynchronously as fast as the network allows.

\section{Specification} \label{spec}

This section presents a full specification of HotStuff based on the basic HotStuff algorithm (Section~\ref{hotstufftheory}), integrating chaining (Section~\ref{chaining}), and a pacemaker (Section~\ref{pacemaker}). The changes made from the original algorithm are informally justified, and a proof of correctness for the modified algorithm is presented.

The consensus module is implemented according to the system architecture described in Section~\ref{architecture}. Messages and requests are received from the main loop, and actions (sending a message, responding to a client request, or resetting the view timer) are outputted while the module updates its own state.

The pseudocode is presented in Algorithms \ref{hotstuffalgorithm} and \ref{pacemakeralgorithm}, with additions coloured in green and modifications in pink. Only the main changes are shown, excluding other features and performance improvements made (such as batching), which are presented in Section~\ref{performance}. The format of the event-driven pseudocode from the original paper is followed to allow for easier comparison and ease of implementation.

\begin{algorithm}[h!]
	\caption{Modified HotStuff}\label{hotstuffalgorithm}
	\begin{algorithmic}[1]
	\Function{CREATE{\large L}EAF}{\textit{parent, cmd, qc}} \label{code_createleaf}
		\color{Magenta}
		\State $\textit{b.parent} \gets \text{branch extending with dummy nodes from}\ \textit{parent}\ \text{to height}\ \textit{curView}$
		\State $\textit{b.height} \gets \textit{curView} + 1$
		\color{black}
		\State $\textit{b.cmd} \gets \textit{cmd}$
		\State $\textit{b.justify} \gets \textit{qc}$
		\State \textbf{return} b
	\EndFunction
	\Procedure{UPDATE}{\textit{b\textsuperscript{*}}}
		\State $\textit{b''} \gets \textit{b\textsuperscript{*}.justify.node}$
		\State $\textit{b'} \gets \textit{b''.justify.node}$
		\State $\textit{b} \gets \textit{b\textsuperscript{*}.justify.node}$
		\State $\text{UPDATE{\large Q}C{\large H}IGH}(\textit{b\textsuperscript{*}.justify})$
		\If{$\textit{b'.height} > \textit{b\textsubscript{lock}.height}$}
			\State $\textit{b\textsubscript{lock}} \gets \textit{b'}$
		\EndIf
		\If{$(\textit{b''.parent} = \textit{b'}) \land (\textit{b'.parent} = \textit{b})$}
			\State $\text{ON{\large C}OMMIT}(\textit{b})$
			\State $\textit{b\textsubscript{exec}} \gets \textit{b}$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large C}OMMIT}{\textit{b}}
		\If{$\textit{b\textsubscript{exec}.height} < \textit{b.height}$}
			\State $\text{ON{\large C}OMMIT}(\textit{b.parent})$
			\State $\text{EXECUTE}(\textit{b.cmd})$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large P}ROPOSAL}{$\text{MSG}\textsubscript{\textit{v}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \textcolor{Magenta}{\textit{qc}})$} \label{code_onreceiveproposal}
		\color{Green}
		\If {$ v = \text{GET{\large L}EADER}(\textit{m.view}) \land \textit{m.view} = curView$} \label{code_checkview}
			\color{black}
			\State $\textit{n} \gets \textit{b\textsubscript{new}.justify.node}$
			\If{$\textit{b\textsubscript{new}.height} > \textit{vheight} \land (\textit{b\textsubscript{new}}\ \text{extends}\ \textit{b\textsubscript{lock}} \lor \textit{n.height} > \textit{b\textsubscript{lock}.height})$}
				\State $\textit{vheight} \gets \textit{b\textsubscript{new}.height}$
				\State $ \text{SEND}(\text{GET{\large L}EADER}(), \text{VOTE{\large M}SG}\textsubscript{\textit{u}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \bot))$
			\EndIf
			\State $\text{UPDATE}(\textit{b\textsubscript{new}})$
			\color{Green}
			\If{$ \text{not}\ \text{IS{\large N}EXT{\large L}EADER}() $} \label{code_proposaltransition}
				\State $\text{ON{\large N}EXT{\large S}YNC{\large V}IEW}(\textit{curview} + 1)$
			\EndIf
		\EndIf
		\color{black}
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large V}OTE}{$\text{VOTE{\large M}SG}\textsubscript{\textit{v}}(\text{GENERIC{\large A}CK}, \textit{b}, \bot)$} \label{code_onreceivevote}
		\color{Green}
		\If {$ \text{IS{\large L}EADER}(\textit{m.view} + 1) \land \textit{m.view} \ge curView$} \label{code_checkleader}
			\color{black}
			\If {$\exists(v, \sigma') \in V\textcolor{Magenta}{\textsubscript{m.view}}[b]$}
				\State \textbf{return}
			\EndIf
			\State $V[b] \gets V\textcolor{Magenta}{\textsubscript{m.view}}[b] \cup \{(v, m.partialSig)\}$
			\If{$ |V\textcolor{Magenta}{\textsubscript{m.view}}[b]| \ge n - f$}
				\State $qc \gets QC(\{ \sigma | (v', \sigma) \in V\textcolor{Magenta}{\textsubscript{m.view}}[b]\})$
				\State $ \text{UPDATE{\large Q}C{\large H}IGH}(\textit{qc}) $
				\color{Green}
				\State $\text{ON{\large N}EXT{\large S}YNC{\large V}IEW}(\textit{m.view} + 1)$ \label{code_gotvotes}
				\color{black}
			\EndIf
		\EndIf
	\EndProcedure
	\Function{ON{\large P}ROPOSE}{$ \textit{b\textsubscript{leaf}}, \textit{cmd}, \textit{qc\textsubscript{high}}$} \label{code_onpropose}
		\State $b\textsubscript{new} \gets \text{CREATE{\large L}EAF}(\textit{b\textsubscript{leaf}, \textit{cmd}, \textit{qc\textsubscript{high}}, \textit{b\textsubscript{leaf}.height} + 1})$
		\State $ \text{BROADCAST}(\text{MSG}\textsubscript{\textit{v}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \textcolor{Magenta}{\textit{qc\textsubscript{high}}})) $ \label{code_propose}
		\State \textbf{return} b\textsubscript{new}
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h!]
	\caption{Modified Pacemaker}\label{pacemakeralgorithm}
	\begin{algorithmic}[1]
	\Function{GET{\large L}EADER}{} \label{code_getleader}
		\color{Green}
		\State $\textbf{return}\ \textit{curView}\ \text{mod}\ \textit{nodeCount}$
		\color{black}
	\EndFunction
	\Procedure{UPDATE{\large Q}C{\large H}IGH}{$\textit{qc'\textsubscript{high}}$}
		\If {$\textit{qc'\textsubscript{high}.node.height} > \textit{qc\textsubscript{high}} $}
			\State $ \textit{qc'\textsubscript{high}} \gets \textit{qc\textsubscript{high}} $
			\State $ \textit{b\textsubscript{leaf}} \gets \textit{qc'\textsubscript{high}.node}$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large B}EAT}{$\textit{cmd}$}
		\If {$ \textit{u} = \text{GET{\large L}EADER}()$}
			\State $ \textit{b\textsubscript{leaf}} \gets \text{ON{\large P}ROPOSE}(\textit{b\textsubscript{leaf}}, \textit{cmd}, \textit{qc\textsubscript{high}})$
		\EndIf
	\EndProcedure
	\Procedure{ON{\large N}EXT{\large S}YNC{\large V}IEW}{$\textit{view}$} \label{code_onnextsyncview}
		\color{Green}
		\State $ \textit{curView} \gets \textit{view}$
		\State $ \text{RESET{\large T}IMER}(\textit{curView})$
		\State $ \text{ON{\large B}EAT}(\textit{cmds.take()})$ \label{code_takecmd}
		\color{black}
		\State $\text{SEND}(\text{GET{\large L}EADER}(), \text{MSG}\textsubscript{\textit{u}}(\text{NEW{\large V}IEW}, \bot, \textit{qc\textsubscript{high}}))$
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large N}EW{\large V}IEW}{$ \text{MSG}\textsubscript{\textit{u}}(\text{NEW{\large V}IEW}, \bot, \textit{qc'\textsubscript{high}}) $}
		\State $ \text{UPDATE{\large Q}C{\large H}IGH}(\textit{qc'\textsubscript{high}}) $
	\EndProcedure
	\color{Green}
	\Procedure{ON{\large R}ECIEVE{\large C}LIENT{\large R}EQUEST}{\text{REQ}(\textit{cmd})} \label{code_onreceiveclientreq}
		\State $ \textit{cmds.add(cmd)} $
	\EndProcedure
	\Procedure{ON{\large T}IMEOUT}{$ \textit{view} $} \label{code_ontimeout}
		\State $ \text{SEND}(\text{GET{\large N}EXT{\large L}EADER}(), \text{MSG}(\text{COMPLAIN}, \bot, \bot)) $ \label{code_timeout}
		\State $ \text{RESET{\large T}IMER}(\textit{view} + 1) $
	\EndProcedure
	\Procedure{ON{\large R}ECIEVE{\large C}OMPLAIN}{$ m = \text{MSG}(\text{COMPLAIN}, \bot, \bot) $} \label{code_onreceivecomplain}
		\If {$ \text{IS{\large L}EADER}(\textit{m.view} + 1) \land \textit{m.view} \ge \textit{curView}$}
			\If {$\exists(v, \sigma') \in C\textsubscript{m.view}[b]$}
				\State \textbf{return}
			\EndIf
			\State $C\textsubscript{m.view}[b] \gets C[b] \cup \{(v, m.partialSig)\}$
			\If{$ |C\textsubscript{m.view}[b]| = n - f$}
				\State $qc \gets QC(\{ \sigma | (v', \sigma) \in C\textsubscript{m.view}[b]\})$
				\State $\text{BROADCAST}(\text{MSG}(\text{NEXT{\large V}IEW}, \bot, \textit{qc}))$ \label{code_nextviewmsg}
			\EndIf
		\EndIf
	\EndProcedure
	\Procedure{ON{\large R}ECEIVE{\large A}NY}{$ m = \text{MSG}(*, *, \textit{qc}) $} \label{code_onreceiveany}
		\If {$ \textit{qc.view} \ge \textit{curView}$}
				\State $\text{ON{\large N}EXT{\large S}YNC{\large V}IEW}(\textit{qc.view} + 1)$ \label{code_gotqc}
		\EndIf
	\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Changes to the original algorithm}
This section informally justifies some of the key changes made to the original algorithm, with a specific focus on the changes I made to integrate the pacemaker with HotStuff (Section~\ref{pacemaker}). Many of these changes were made to fix deadlocks encountered during implementation.

\begin{description}
	\item \textit{Algorithm \ref{hotstuffalgorithm}, line \ref{code_proposaltransition}} --- if a replica is the next leader, it waits to receive votes before transitioning to the next view. This prevents a deadlock where the leader transitions to the next view too early and ignores vote messages from an earlier view.
	\item \textit{Algorithm \ref{hotstuffalgorithm}, line \ref{code_checkleader}} --- a node collects vote messages from future views so that if it has fallen behind, it can receive a quorum of votes and catch up to the current view. This prevents an honest node from falling behind and not being able to make progress in the view where it is the leader. Votes from different future views are stored in separate sets ($V\textcolor{Magenta}{\textsubscript{m.view}}$), to prevent votes from different views being used to form a QC.
	\item \textit{Algorithm \ref{hotstuffalgorithm}, line \ref{code_onpropose}} --- proposals now include a QC, allowing replicas to catch up if they are in a lower view.
	\item \textit{Algorithm \ref{pacemakeralgorithm}, line \ref{code_getleader}} --- a round-robin system to assigns leaders to views.
	\item \textit{Algorithm \ref{pacemakeralgorithm}, line \ref{code_onnextsyncview}} --- as soon as a node transitions into a view where it is leader, ON{\large B}EAT is invoked, causing it to propose a new value.
	\item \textit{Algorithm \ref{pacemakeralgorithm}, line \ref{code_onreceiveany}} --- if a node receives any QC from a future view $v$, it can safely transition to view $v + 1$.
\end{description}

\subsection{Proofs} \label{proof}

In this section, the correctness of the specification is proven. The safety (Property~\ref{safetyproperty}) of the specification holds trivially, as the changes that have been made do not invalidate the argument for the safety of the generic Byzantine algorithm (Section~\ref{safetyargument}). Similarly, liveness (Property \ref{livenessproperty}) still holds from the previous argument (Section~\ref{livenessargument}). The only thing that remains to be proven is that the pacemaker has the properties that are needed to provide the conditions for liveness, as discussed in Section~\ref{pacemaker}. These properties ensure that all honest replicas will remain in some view with an honest leader for long enough to make progress.

For this proof, the consensus machine (Algorithm~\ref{hotstuffalgorithm}) and the pacemaker (Algorithm~\ref{pacemakeralgorithm}) are considered to be separate entities that are scheduled fairly. Furthermore, the assumption is made that GST has been reached and messages have a bounded latency of $\Delta$ (Assumption~\ref{partialsyncassumption}).

\begin{theorem}[View synchronisation] \label{viewsync}
	There exist infinite views $v_k$ and time intervals $\mathcal{I}_k$ such that the following hold:
	\begin{enumerate}
		\item The leader of $v_k$ is honest.
		\item All honest replicas are in view $v_k$ for the duration of $\mathcal{I}_k$.
		\item $\mathcal{I}_k$ is long enough for the replicas to make progress
	\end{enumerate}
\end{theorem}

\begin{lemma} \label{viewslemma}
	There exists an infinite number of consecutive assignments of two honest leaders to views. That is, we can always find future consecutive views $v_1$ and $v_2$ with honest leaders.
\end{lemma}

\begin{figure}[h!]
	\centering
	\includegraphics[]{lemma1.pdf}
	\caption{Example of round-robin leader allocation for $f = 3$. Red rectangles denote byzantine leaders.}
	\label{lemma1diagram}
\end{figure}

\begin{proof}
	A round-robin system allocates leaders to views. We proceed by contradiction; assume that the view of every honest leader is followed by a byzantine one (Figure~\ref{lemma1diagram}). There are $2f + 1$ honest nodes, so there must be $2f + 1$ byzantine ones. However, there are only $f$ byzantine nodes - a contradiction.
\end{proof}

\begin{lemma} \label{progressionlemma}
	Every honest replica $x$ will eventually become the leader of a view.
\end{lemma}

\begin{proof}
	If a byzantine leader tries to prevent honest nodes from transitioning to a higher view, the honest nodes will eventually timeout and send a COMPLAIN message to the next leader (Algorithm~\ref{pacemakeralgorithm}, line~\ref{code_timeout}). This may repeat if the next leader is also byzantine. By Lemma~\ref{viewslemma}, the COMPLAINs will eventually be sent to an honest leader that will send a NEXT{\large V}IEW message and transition all replicas into a new view (Algorithm~\ref{pacemakeralgorithm}, line~\ref{code_nextviewmsg}). Since $x$ will always progress to a higher view, it will eventually reach a view where it is the leader.
\end{proof}

\begin{proof}[Proof of Theorem \ref{viewsync}]
	From Lemma~\ref{viewslemma} we have that we can always find future views $v_1$ and $v_2$ with honest leaders $l_1$ and $l_2$, and from Lemma~\ref{progressionlemma} we have that $l_1$ will eventually enter $v_1$. We argue that all honest replicas will simultaneously be in either $v_1$ or $v_2$. Consider the cases of how $l_1$ could have entered $v_1$:
	\begin{enumerate}
		\item $l_1$ received a quorum of votes (Algorithm~\ref{hotstuffalgorithm}, line \ref{code_gotvotes}) --- $l_1$ will broadcast a QC of votes that will be received by all honest replicas within $\Delta$. These replicas will transition to $v_2$, and send a vote to $l_2$ which will also transition once it receives a quorum of votes. Hence all honest replicas will simultaneously be in $v_2$.
		\item $l_1$ receives a QC of COMPLAINS in a NEXT{\large V}IEW message (Algorithm~\ref{hotstuffalgorithm}, line~\ref{code_gotqc}) --- since $l_1$ is the leader, it must have been the node broadcast that the NEXT{\large V}IEW message to all honest replicas; they will receive it within $\Delta$ and all enter $v_1$ simultaneously.
	\end{enumerate}
	From this, we have that there is some time interval $\mathcal{I}_k$ throughout which all honest replicas are in view $v_k$, which has an honest leader. Once the replicas have entered $v_k$ they will only transition to the next view once they have made progress, or once they timeout, which will not happen assuming the timeout is sufficiently long. Hence $\mathcal{I}_k$ is long enough for the replicas to make progress.
\end{proof}

\begin{theorem}[Synchronisation validity] \label{syncvalid}
	The pacemaker advances the view only if at least one honest consensus machine requests it to be advanced.
\end{theorem}

\begin{proof}
	This holds trivially for the calls to ON{\large N}EXT{\large S}YNC{\large V}IEW in Algorithm~\ref{hotstuffalgorithm}, as the view is advanced on the request of the consensus machine.

	The only other way the view can be advanced is on the receipt of a QC (Algorithm~\ref{pacemakeralgorithm}, line \ref{code_gotqc}). For a QC to be formed a quorum of $n - f$ nodes must have complained or voted; at least one of these must have been an honest consensus machine that requested for the view to be advanced.
\end{proof}

% https://decentralizedthoughts.github.io/2021-09-30-distributed-consensus-made-simple-for-real-this-time/
\section{Practical challenges and optimisations} \label{performance}

This section presents solutions to some of the practical challenges of implementing HotStuff and describes optimisations I made to improve performance. Developing these solutions was non-trivial; it involved extensive analysis of the program trace and timing data, and comparing the performance of different prototypes.

One main practical consideration was designing the types and structure of the core consensus module (Section~\ref{architecture}), to implement the algorithm given in the specification (Section~\ref{spec}). Recall that this module takes as input messages and client requests (\textit{events}), and returns actions (such as sending a message) and an updated consensus state. I used \textit{variants} to tag the different types of events, with attached \textit{records} to store the body of the event (such as the node and QC of a proposal). The core algorithm is expressed as a \textit{match} statement to handle each type of event. The output from this is a list of actions (also implemented as variants) and a new state.

\subsection{Batching} \label{batching}
This section discusses the practical challenges of implementing effective \textit{batching}. Batching is a standard technique to improve the goodput (requests processed per second) of a consensus algorithm by making each node contain multiple commands instead of just one. This allows a single view to commit multiple commands.

A naive implementation of batching is simple to implement; instead of taking a single command from the queue to propose (Algorithm~\ref{pacemakeralgorithm}, line~\ref{code_takecmd}), the whole queue can be batched into a single proposal. Analysis of the timing data for the naive implementation showed that it dramatically increased latency.

The rest of this section describes further optimisations I made to make batching effective.

\subsubsection{Filtering commands from batches} \label{filtering}
One optimisation that I implemented to make batching effective is filtering incoming commands to prevent them from being proposed by multiple nodes. This is achieved by nodes maintaining a set of commands that they have seen in the proposals of other nodes, and filtering these commands from their proposal. This optimisation increased the effectiveness of batching, which will be demonstrated in an ablation study (Section~\ref{ablation}).

\begin{algorithm}[h!]
	\caption{Filtering implementation} \label{dedup}
	\begin{algorithmic}[1]
	\Procedure{ON{\large R}ECEIVE{\large P}ROPOSAL}{$\text{MSG}\textsubscript{\textit{v}}(\text{GENERIC}, \textit{b\textsubscript{new}}, \textit{qc})$}
		\If {$ v = \text{GET{\large L}EADER}(\textit{m.view}) \land \textit{m.view} = curView$}
			\State $ \textit{seen} \gets \textit{seen} \cup \textit{b\textsubscript{new}.cmds}$
			\State \textcolor{gray}{//\ \dots}
		\EndIf
	\EndProcedure
	\Procedure{ON{\large N}EXT{\large S}YNC{\large V}IEW}{$\textit{view}$}
		\State $ \textit{curView} \gets \textit{view}$
		\State $ \text{ON{\large B}EAT}(\textit{cmds} \setminus \textit{seen})$
		\State $ \textit{cmds} = \textit{seen} = \{\}$ \label{smalloptimisation}
		\State \textcolor{gray}{//\ \dots}
	\EndProcedure
	\Procedure{ON{\large R}ECIEVE{\large C}LIENT{\large R}EQUEST}{\text{REQ}(\textit{cmd})}
		\State $ \textit{cmds} \gets \textit{cmds} \cup \{\textit{cmd}\} $
	\EndProcedure
	\end{algorithmic}
\end{algorithm}

Pseudocode for an implementation of filtering is given in Algorithm~\ref{dedup}. Sets are used to store both the commands that are waiting to be proposed and those that have been seen so that the difference can be efficiently computed. Also note the small optimisation on line \ref{smalloptimisation}: the seen set can safely be emptied as the commands have already been filtered, reducing the amount of computation required to calculate the set difference next time.

This optimisation is effective as the load generator is send-to-all (Section~\ref{loadgenerator}), so a new command is added to the queue of all nodes, and may be included in the proposals of different nodes. Filtering out commands so that they are not proposed multiple times follows the general design principle:

\textbf{Design principle: } Attempt to minimise the amount of redundant work that the system carries out by screening incoming work to check that it needs to be done.

An alternative implementation of filtering that was prototyped was maintaining a set of commands that had been committed rather than seen, but this filtered out far fewer commands and did not significantly improve performance. This is likely because it takes several views for a command to be committed, and in this time multiple nodes may propose the same command and it will not be filtered.

\subsubsection{Batch sizes} \label{batchsizes}
I further improved the effectiveness of batching by limiting the size of each batch. I demonstrate that this approach improves performance in a study of the system with different batch size limits (Section~\ref{batchsizeseval}).

Implementing this feature requires minimal changes to Algorithm~\ref{dedup}, one simply has to take a subset of \textit{cmds} to propose instead of the whole set. It is important to take the oldest commands from \textit{cmds} to include in the proposal, so that older commands are not starved by newer ones, resulting in high latency. This can be accomplished by using an ordered set (such as a tree set) and ordering by command identifiers, which are ascending integers in my implementation.

The optimisation is effective because at higher message sizes the overheads of Cap'n Proto increase (Section~\ref{capnpbenchmark}), so smaller batches can lead to better performance. There is an inherent trade-off between increasing batch size to commit more commands, and messages becoming slower due to increased overheads.

\subsection{Node chains}
This section concerns the challenges of designing a suitable type for node chains (nodes\footnote{Node is another word for node chain in this context, not to be confused with a node in the system.}) that can be efficiently serialised by Cap'n Proto and sent over the network. I will discuss the challenges of designing the node type that must be overcome to allow the system to operate, then describe further optimisations that can be made. Recall from the discussion of the system architecture (Section~\ref{architecture}) that the internal node type must be converted into a Cap'n Proto type in order to be serialised.

My initial naive implementation of the node type was an OCaml record that contained the fields \textit{parent}, \textit{cmds}, \textit{height}, and \textit{justify}. The \textit{justify} field contained a QC that contains another node inside it.

\subsubsection{The problem with recursive types}
One problem encountered in converting internal types to Cap'n Proto types was that some nodes are recursive; the node stored inside the \textit{justify} field points to themselves. This is the case in the chained HotStuff algorithm (Section~\ref{chaining}); it has a recursive genesis node \textit{b\textsubscript{0}} that starts the chain of nodes. The genesis node \textit{b\textsubscript{0}} contains a hardcoded link to itself, so $ b_0.\textit{justify}.\textit{node} = b_0$.

This recursion poses a problem when carrying out the conversion between Cap'n Proto types and OCaml types. It is perfectly possible to define a recursive type in OCaml, so one can represent \textit{b\textsubscript{0}} inside the consensus state machine. However, the naive implementation of a function to convert this node into a Cap'n Proto type will not terminate, as it will infinitely recurse into the field $ b_0.\textit{justify}.\textit{node} $.

A simple solution to this problem is to add a flag to the Cap'n Proto schema \textit{is\_b\textsubscript{0}}; when this flag is enabled then the node is assumed to be equal to \textit{b\textsubscript{0}}. This prevents \textit{b\textsubscript{0}} from ever having to be converted into a Cap'n Proto type or being sent over the network, it can instead be reconstructed as a recursive type in the consensus state machine of the receiver.

\subsubsection{Replacing the justify.node field with an offset}
One problem of the naive node type implementation was that it caused the system to crash due to rapidly increasing memory usage; this can be prevented by replacing the \textit{node} record inside the \textit{node.justify.node} field with an integer offset into the chain. This dramatically decreased the memory usage of the function to convert from internal types to Cap'n Proto types. The inefficiency of this function was revealed by profiling the memory usage of the program using Memtrace  (Section~\ref{testing}).

The source of this problem was the inefficient design of the \textit{node.justify} field, which contained a whole node inside it. As shown in Figure~\ref{nchain}, each node has two links to previous nodes in the chain through the \textit{parent} field and the \textit{node.justify.node} field. By having each of these fields contain a whole \textit{node} record, much of the chain had multiple redundant copies, resulting in a very bloated \textit{node} object that was very expensive to convert.

A solution to this problem is to store an integer offset to a node inside the \textit{justify} field rather than a \textit{node} record. This offset represents how many \textit{parent} links away the node is, and so can be used to reconstruct all of the original information. To implement this another type \textit{node\_justify} was added, which is identical to \textit{qc}, but with the field \textit{node} replaced with \textit{node\_offset}. One must then convert between the \textit{node\_justify} and \textit{qc} types to reconstruct the original data and follow the \textit{node.justify.node} link.

\subsubsection{Optimising the node equality function} \label{equality}
One optimisation of the node type that was implemented is storing a \textit{digest} field in the node that is a hash over all of the other fields\footnote{Notably the hash can be computed over the \textit{digest} field of the parent node rather than recursing through the whole chain.}, enabling efficient equality checking of nodes. This means that two nodes can be compared by their digests without having to recurse through the entire chain; the digests being equal cryptographically guarantees that the whole chains are equal. The need for this optimisation was discovered by profiling the memory usage of the node equality function using Memtrace.

\subsubsection{Optimising by truncating nodes} \label{truncation}
To optimise the sending of nodes I implemented node truncation; this reduces the size of nodes being sent over the network by cutting off older parts of the chain that the receiver already knows about. This is effective because the size of messages being sent is a bottleneck in my implementation (Section~\ref{capnpbenchmark}). The effectiveness of this optimisation is demonstrated in an ablation study (Section~\ref{ablation}).

To truncate the node, my implementation recurses into the node's \textit{parent} field, then deletes the field at some chosen depth. The entire node can then be reconstructed at the receiver, by `splicing' it back together with \textit{b\textsubscript{exec}}, which contains the node up to the point that has been executed. Splicing together the nodes is done by recursing into the truncated node until it is equal to \textit{b\textsubscript{exec}}, then setting the deleted parent field to \textit{b\textsubscript{exec}}. The node equality function will still work on truncated nodes because of my optimisation to use digests (Section~\ref{equality}); a node will still have the same digest once it is truncated.

One practical challenge of this approach is choosing a suitable depth such that there is enough information at the receiver to reconstruct the whole chain. If there is a gap between the truncated node and \textit{b\textsubscript{exec}} at the receiver, this will lead to commands being missed out and not executed. This is a problem if a node becomes isolated from the rest; it must be able to catch up to the others once the network partition is healed.

To overcome this problem I used a TCP-style approach. I included a field containing the height of the \textit{b\textsubscript{exec}} node to the \textit{propose}, \textit{new-view}, and \textit{complain} messages. Each node maintains a list of the \textit{b\textsubscript{exec}} height of every other node. When making a proposal, the leader takes the minimum height from this list and truncates the node up to that depth. This ensures that every node that receives the proposal has enough information to reconstruct the entire log.

There are some cases when the leader does not receive the latest \textit{b\textsubscript{exec}} of every other node before it proposes. This means that the leader will not truncate the node as much as it could have. I optimised this by having a node send the entire list of all stored \textit{b\textsubscript{exec}} heights rather than just its own, allowing the heights to propagate around the system more quickly.

% \subsection{Connections}
% talk about capabilities?
% when to set up connections? use promises
% https://outlook.office.com/mail/id/AAQkADc5ODY2YTdlLTQ3NGMtNDVmOS05ZDAxLWNhYzkwMjU2MThhZAAQAEXEcHtcUNFFo%2FRcUtQcSmc%3D

\section{Implementing for evaluation} \label{benchcode}
% Benchmarking code
% discuss open loop vs closed loop clients
% avoid coordinated ommision by using open loop clients
% section 3: https://dl.acm.org/doi/pdf/10.1145/3447851.3458739

This section describes the infrastructure that will be used to evaluate system performance in Chapter~\ref{evaluation}, including the scripting developed to automate running experiments.

\subsection{Load generator} \label{loadgenerator}

\begin{figure}[h]
\centering
\includegraphics[]{openloop.pdf}
\caption{Open-loop load generator.}
\label{openloop}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[]{closedloop.pdf}
\caption{Closed-loop load generator.}
\label{closedloop}
\end{figure}

The load generator is responsible for sending client requests to the nodes of the system. One can vary the throughput that the load generator drives the system at, and the duration that it runs for before it sends \textit{kill} messages to the nodes, ending the test. It is also responsible for timing and calculating statistics.

\begin{description}
	\item \textit{Throughput} --- the number of requests sent by the load generator each second.
	\item \textit{Goodput} --- the number of requests that are responded to each second. This is calculated as the number of responses divided by the time difference between the first response and the end of the test.
	\item \textit{Latency} --- the amount of time it takes between sending a request and receiving a response. The load generator reports the mean and standard deviation of latencies.
\end{description}

The load generator is open-loop (Figure~\ref{openloop}), which means that it dispatches a request every $\delta$ seconds for the duration of the experiment, where $\delta = \frac{1}{\textit{throughput}}$. This is in contrast to a closed-loop generator (Figure~\ref{closedloop}), which must wait until it receives a response before sending the next request. An open-loop load generator is more useful as it allows us to overload the system and test its limits, whereas a closed-loop load generator waits for responses from the system, so cannot overload it.

The load generator is send-to-all, meaning that a command is sent to all nodes. An alternative is send-to-one, where a command is sent to a single node which is chosen at random. Send-to-all reduces latency as the next leader will have been sent the command, and may choose to propose it. In send-to-one it may take several views until the node that the request was sent to becomes the leader.

The load generator uses Lwt to asynchronously dispatch requests and stores a promise that will be fulfilled with their response. In the case of send-to-all there are promises waiting for a response from each node that are combined using \texttt{Lwt.pick}, meaning that the first node to respond will fulfil the promise and other responses will be ignored. Before beginning the experiment the load-generator sends `dummy' requests to each node until all of them have sent a response; this ensures that all nodes are properly up and running before the experiment begins, reducing start-up effects.

\subsection{Experiment scripts} \label{experimentscripts}
Python scripts are used to automate the running of experiments. These scripts start the nodes and the load generator, wait for the experiment to run, kill the processes, run a script to plot graphs, and then start the next experiment.

Different experiments may vary input variables such as throughput, batch size, and number of nodes. The script takes every permutation. Each experiment is repeated several times to reduce variance. Experiments are run in a random order so that if there is interference for some part of the test, this is not correlated with the parameters of the experiment, making anomalies easier to spot.

\subsection{Logging framework}
I developed a logging framework that stores the time taken for important parts of the program to execute and outputs key statistics such as the mean and standard deviation at the end of the test. This was essential for analysing the performance of the system to develop the optimisations described in Section~\ref{performance}.

The logging framework helped to reduce the effect of probing effects, where the behaviour of a system is altered by the act of measuring it. My previous approach printed the time taken throughout the execution of the program; since printing is CPU-heavy this resulted in probing effects. Notably, the print statements were not in-between the statements that measured the time taken, but they caused delays to happen in other parts of the program (presumably when the buffers were being flushed).

\textbf{Design principle: } Minimise probing effects by carrying out the minimum possible amount of work in critical areas of the program by storing data and moving work (such as outputting statistics) to less critical areas of the program.

\section{Repository Overview} \label{repo}

\begin{small}
\begin{forest}
	for tree={font=\sffamily, grow'=0,
	folder indent=0.9em, folder icons,
	edge=densely dotted}
	[src
		[\ bin,
			[load\_gen.ml, is file]
			[run\_node.ml, is file]
		]
		[\ consensus,
			[consensus\_chained\_impl.ml, is file]
			[consensus\_impl.ml, is file]
			[consensus.ml, is file]
			[consensus.mli, is file]
			[types.ml, is file]
			[types.mli, is file]
			[util.ml, is file]
		]
		[\ experiments,
			[\ data]
			[\ graphs]
			[plot.py, is file]
			[runExperiments.py, is file]
		]
		[\ lib,
			[action\_handler.ml, is file]
			[api\_wrapper.ml, is file]
			[api\_wrapper.mli, is file]
			[hs\_api.capnp, is file]
			[lib.ml, is file]
			[lib.mli, is file]
			[main\_loop.ml, is file]
			[msg\_sender.ml, is file]
			[server.ml, is file]
			[types.ml, is file]
			[util.ml, is file]
		]
		[README.md, is file]
	]
\end{forest}
\end{small}

To run an experiment, follow the instructions in \textit{README.md} to set up your environment. You can then run experiments by executing \texttt{python3 runExperiments.py}, and modify this script to vary the input parameters such as throughput and experiment time.