% This chapter is likely to be very short and it may well refer back to the Introduction. It might offer a reflection on the lessons learned and explain how you would have planned the project if starting again with the benefit of hindsight.

% general-use framework ??
% \section{Future Work}
% Further work would port to using the async library which is known to have better performance. It was out of scope to rewrite in async or use it in the first place due to poor documentation (although now I could look at the type signatures and understand the documentation). Hopefully reimplementing would give better performance and avoid the bugs of capnrpc. I would carry out more extensive tests on the message sending capabilities before diving into implementation. I would be more aware beforehand of the whole algorithm (including the pacemaker code) and implement based on the new pseudocode we have presented and proven correct. This would allow for better structuring of the code.

% if i was to reimplement i would already have a solid grasp on the algorithm, language, and frameworks. i could focus more on the practicalities of bottlenecks, profiling, testing lwt & queueing, benchmarking, etc. from the outset

% to develop a real production ready system that has a reasonable level of performance would take an order of magnitude more time than this problem. one would have to carefully study byzantine threats and how one would counter availability attacks. In some cases optimisations may be antagonistic with security considerations (eg. TCP style truncation could be attacked). if the system was to be deployed in a real environment (eg. a cryptocurrency) security would be paramount, and it could take years of fixes and bug bounties to develop a robust system.

% We have presented a potential path for implementing verifiable anonymous identities and reconfiguration using our permissioned blockchain, future work could consist of a practical implementation of this.

[consistent tense???]
In this project we have given a reference implementation of the HotStuff byzantine consensus algorithm in OCaml, contributing to the wider OCaml ecosystem. The core algorithm is written in a self-contained module which could be reused by other projects with differing architectures and RPC systems. We have described some of the practical challenges of implementing HotStuff and implemented several optimisations of the basic algorithm (section \ref{performance}). One main challenge was adapting the HotStuff pacemaker; we gave a full specification and proved that our pacemaker has desirable properties (section \ref{pacemaker}).

We have successfully met the requirements set out in section \ref{requirements}. There is significant evidence for the correctness of our implementation; our testing suite has 100\% coverage of the consensus state machine code (section \ref{testing}). Evaluation of the system was carried out both locally and on a simulated network, and we analysed its behaviour with different parameters, and under varying conditions (section \ref{evaluation}). This analysis helped to identify that the system bottlenecks are message serialisation and cryptography. We also implemented several optimisations (section \ref{performance}), and demonstrated their effectiveness in an ablation study (section \ref{ablation}).

Given that message serialisation and cryptography were shown to be system bottlenecks, future work could aim to overcome some of these problems to achieve better performance. One potential direction would be to implement custom message serialisation, and use a faster RPC library such as EIO \cite{noauthor_eio_2023}; both of these were out of the scope of this project. Alternative cryptography libraries could also be explored, although this may be an inherent bottleneck of any HotStuff implementation.

Future work could also explore the challenges of deploying a production-ready system based on our implementation. Although HotStuff is byzantine-fault tolerant, this project has not considered other security threats such as attacks on availability. Solving these problems would be highly non-trivial; some of our optimisations may be antagonistic with security considerations, for example a malicious node could repeatedly request the whole chain to be sent (instead of truncated) and bring down the system. One could also implement a proof of work or proof of stake mechanism on top of our implementation to make it resilient to Sybil attacks, allowing it to be deployed in a permissionless setting.

One lesson I learnt from this project is that graphs are an invaluable tool for analysing the performance of a distributed algorithm, and analysis of graphs should be included in the iterative passes of the spiral development model (section \ref{devmethods}). Much of development time was spent debugging system performance to implement the optimisations described in section \ref{performance}. I found that as I was creating graphs and analysing performance (section \ref{hotstuffbenchmarks}), I was able to more quickly find bugs and intuitively understand the behaviour of the system. Therefore if I were to do a similar project in future, I would further automate and parallelise the testing and graph plotting scripts to quickly gain insight during development.

In conclusion, this project has provided an implementation of a byzantine consensus algorithm, a key algorithm in the development of decentralised software. Decentralised software has far-reaching implications, and could challenge the control of large centralised authorities over critical infrastructure, platforms, and organisations.