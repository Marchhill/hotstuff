% This chapter is likely to be very short and it may well refer back to the Introduction. It might offer a reflection on the lessons learned and explain how you would have planned the project if starting again with the benefit of hindsight.

% general-use framework ??
% \section{Future Work}
% Further work would port to using the async library which is known to have better performance. It was out of scope to rewrite in async or use it in the first place due to poor documentation (although now I could look at the type signatures and understand the documentation). Hopefully reimplementing would give better performance and avoid the bugs of capnrpc. I would carry out more extensive tests on the message sending capabilities before diving into implementation. I would be more aware beforehand of the whole algorithm (including the pacemaker code) and implement based on the new pseudocode we have presented and proven correct. This would allow for better structuring of the code.

% if i was to reimplement i would already have a solid grasp on the algorithm, language, and frameworks. i could focus more on the practicalities of bottlenecks, profiling, testing lwt & queueing, benchmarking, etc. from the outset

% to develop a real production ready system that has a reasonable level of performance would take an order of magnitude more time than this problem. one would have to carefully study byzantine threats and how one would counter availability attacks. In some cases optimisations may be antagonistic with security considerations (eg. TCP style truncation could be attacked). if the system was to be deployed in a real environment (eg. a cryptocurrency) security would be paramount, and it could take years of fixes and bug bounties to develop a robust system.

% We have presented a potential path for implementing verifiable anonymous identities and reconfiguration using our permissioned blockchain, future work could consist of a practical implementation of this.

In this project I have given the first reference implementation of the HotStuff byzantine consensus algorithm in OCaml, contributing to the wider OCaml ecosystem\footnote{Since the project is implemented purely in OCaml, it could be deployed in a MirageOS unikernel~\cite{mirage}}. The core algorithm is written in a module that could be reused by other projects with differing architectures and RPC systems. I solved practical challenges of implementing HotStuff and implemented several optimisations of the basic algorithm (Section~\ref{performance}). One main challenge was adapting the HotStuff pacemaker; I gave a full specification and proved its correctness (Section~\ref{pacemaker}).

The project successfully meets the requirements set out in Section~\ref{requirements}. There is significant evidence for the correctness of my implementation; the testing suite has 100\% coverage of the consensus state machine code (Section~\ref{testing}). Evaluation of the system was carried out both locally and on a simulated network, and I analysed its behaviour with different parameters, and under varying conditions (Chapter~\ref{evaluation}). This analysis helped to identify that the system bottlenecks are Cap'n Proto overheads and cryptography. I also implemented several optimisations (Section~\ref{performance}) and demonstrated their effectiveness in an ablation study (Section~\ref{ablation}).

Given that Cap'n Proto overheads and cryptography were shown to be system bottlenecks, future work could aim to overcome some of these problems to achieve better performance. One potential direction would be to implement custom message serialisation, and use a faster RPC library; both of these were out of the scope of this project. Alternative cryptography libraries could also be explored, although this is an inherent bottleneck of any HotStuff implementation.

Future work could also implement a proof of stake mechanism on top of my implementation to make it resilient to Sybil attacks, allowing it to be deployed in a permissionless setting. As the participants in a permissionless network are not controlled, such a system would have to consider  security threats such as attacks on availability; although HotStuff is byzantine-fault tolerant it provides no protection from these threats. Solving these problems would be non-trivial; some of my optimisations may be antagonistic with security considerations, for example, a malicious node could repeatedly request the whole chain to be sent (instead of truncated) and bring down the system.

One lesson I learnt from this project is that graphs are an invaluable tool for analysing the performance of a distributed algorithm, and analysis of graphs should be included in the iterative passes of the waterfall development model (Section~\ref{devmethods}). Much of development time was spent debugging system performance to implement the optimisations described in Section~\ref{performance}. I found that as I was creating graphs and analysing performance (Section~\ref{hotstuffbenchmarks}), I was able to more quickly find bugs and intuitively understand the behaviour of the system. Therefore if I were to do a similar project in future, I would further automate and parallelise the testing and graph plotting scripts to quickly gain insight during development.

Additionally, I learnt system design principles such as clearing a backlog of work before accepting new tasks and waiting to output logs to avoid probing affects. These principles may be useful to my future projects.

In conclusion, this project has provided an implementation of a byzantine consensus algorithm, a key algorithm in the development of decentralised software. Decentralised software has far-reaching implications, and could challenge the control of large centralised authorities over critical infrastructure, platforms, and organisations.